import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import textwrap

# ==================== è®¾ç½® ====================
# 1. ç¡®ä¿CSVæ–‡ä»¶åœ¨åŒç›®å½•ï¼Œæ–‡ä»¶åä¸ºï¼šscientific_ToggleSwitch_vs_FyneCheckbox_20251221_123455.csv
csv_filename = "scientific_ToggleSwitch_vs_FyneCheckbox_20251221_123455.csv"

# 2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
csv_path = Path(csv_filename)
if not csv_path.exists():
    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_filename}")
    print("è¯·ç¡®ä¿:")
    print("1. CSVæ–‡ä»¶åœ¨å½“å‰ç›®å½•")
    print(f"2. æ–‡ä»¶åæ­£ç¡®: {csv_filename}")
    print("å½“å‰ç›®å½•:", Path.cwd())
    exit()

# ==================== ç¬¬1æ­¥ï¼šè¯»å–æ•°æ® ====================
print(f"ğŸ“ æ­£åœ¨è¯»å–æ•°æ®: {csv_filename}")
try:
    # è¯»å–CSVæ–‡ä»¶
    df = pd.read_csv(csv_filename)
    print(f"âœ… æ•°æ®è¯»å–æˆåŠŸ!")
    print(f"   æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"   åˆ—å: {list(df.columns)}")
except Exception as e:
    print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    exit()

# æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
print("\nğŸ“Š æ•°æ®é¢„è§ˆ:")
print(df.head())

# æ£€æŸ¥ç»„ä»¶åç§°
print(f"\nğŸ” æ•°æ®ä¸­çš„ç»„ä»¶:")
print(df['component_name'].value_counts())

# ==================== ç¬¬2æ­¥ï¼šåˆ†ç¦»æ•°æ® ====================
# æ£€æŸ¥åˆ—åæ˜¯å¦æœ‰ç©ºæ ¼é—®é¢˜
print(f"\nğŸ”§ æ•°æ®å¤„ç†...")
if 'component_name' not in df.columns:
    # å°è¯•ä¿®å¤å¯èƒ½çš„åˆ—åé—®é¢˜
    df.columns = df.columns.str.strip()
    print(f"   ä¿®å¤åçš„åˆ—å: {list(df.columns)}")

# åˆ†ç¦»ä¸¤ç»„æ•°æ®
toggle_data = df[df['component_name'] == 'ToggleSwitch'].copy()
native_data = df[df['component_name'] == 'FyneCheckbox'].copy()

print(f"   ToggleSwitchæ ·æœ¬æ•°: {len(toggle_data)}")
print(f"   FyneCheckboxæ ·æœ¬æ•°: {len(native_data)}")

# æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
if len(toggle_data) == 0 or len(native_data) == 0:
    print("âŒ é”™è¯¯: æŸä¸ªç»„ä»¶çš„æ•°æ®ä¸ºç©º!")
    print(f"   ToggleSwitchæ•°æ®: {len(toggle_data)} è¡Œ")
    print(f"   FyneCheckboxæ•°æ®: {len(native_data)} è¡Œ")
    exit()

# ==================== ç¬¬3æ­¥ï¼šå°æç´å›¾å¯è§†åŒ– ====================
print(f"\nğŸ¨ ç”Ÿæˆå°æç´å›¾...")

# è®¾ç½®å›¾è¡¨é£æ ¼ä¸ºç°åº¦
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette(["#FFFFFF", "#B0B0B0"])  # ç™½è‰²å’Œç°è‰²

# å…¨å±€å­—ä½“æ”¾å¤§
plt.rcParams.update({'font.size': 18, 'axes.titlesize': 22, 'axes.labelsize': 20, 'legend.fontsize': 18, 'xtick.labelsize': 18, 'ytick.labelsize': 18})

# åˆ›å»ºä¸‰ä¸ªå­å›¾ï¼šFPSã€å†…å­˜ã€CPU
fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # åŠ å®½å›¾ç‰‡
# fig.suptitle('Performance Distribution: TS vs FC', fontsize=24, fontweight='bold')

# å‡†å¤‡æ•°æ®ç”¨äºç»˜å›¾
plot_data = []
for idx, component in enumerate(['ToggleSwitch', 'FyneCheckbox']):
    component_df = df[df['component_name'] == component]
    short = 'TS' if component == 'ToggleSwitch' else 'FC'
    for _, row in component_df.iterrows():
        plot_data.append({
            'Group': short,  # æ”¹ä¸ºGroupï¼Œåç»­åªæ˜¾ç¤ºTS/FC
            'FPS': row['fps'],
            'Memory (MB)': row['memory_usage_mb'],
            'CPU (%)': row['cpu_percent']
        })

plot_df = pd.DataFrame(plot_data)

# 1. FPSåˆ†å¸ƒ
sns.violinplot(x='Group', y='FPS', data=plot_df, ax=axes[0], inner='quartile', palette=["#FFFFFF", "#B0B0B0"])
axes[0].set_title('FPS Distribution', fontweight='bold', fontsize=22)
axes[0].set_xticklabels(['TS', 'FC'], fontsize=18)
axes[0].set_ylabel('Frames Per Second', fontsize=20)
axes[0].grid(True, alpha=0.3, linestyle=':')
axes[0].tick_params(axis='both', which='major', labelsize=18)

fps_means = plot_df.groupby('Group')['FPS'].mean()
for i, short in enumerate(['TS', 'FC']):
    axes[0].axhline(y=fps_means[short], color='black', linestyle='--', 
                    alpha=0.7, linewidth=1.5)
    axes[0].text(i+0.4, fps_means[short], f'Î¼={fps_means[short]:.2f}', 
                 fontsize=15, fontweight='bold', color='black',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='#E0E0E0', edgecolor='black', alpha=0.7))

# 2. å†…å­˜åˆ†å¸ƒ
sns.violinplot(x='Group', y='Memory (MB)', data=plot_df, ax=axes[1], inner='quartile', palette=["#FFFFFF", "#B0B0B0"])
axes[1].set_title('Memory Usage Distribution', fontweight='bold', fontsize=22)
axes[1].set_xticklabels(['TS', 'FC'], fontsize=18)
axes[1].set_ylabel('Memory Usage (MB)', fontsize=20)
axes[1].grid(True, alpha=0.3, linestyle=':')
axes[1].tick_params(axis='both', which='major', labelsize=18)

mem_means = plot_df.groupby('Group')['Memory (MB)'].mean()
for i, short in enumerate(['TS', 'FC']):
    axes[1].axhline(y=mem_means[short], color='black', linestyle='--', 
                    alpha=0.7, linewidth=1.5)
    axes[1].text(i+0.4, mem_means[short]+0.05, f'Î¼={mem_means[short]:.2f}', 
                 fontsize=15, fontweight='bold', color='black',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='#E0E0E0', edgecolor='black', alpha=0.7))

# 3. CPUåˆ†å¸ƒ
sns.violinplot(x='Group', y='CPU (%)', data=plot_df, ax=axes[2], inner='quartile', palette=["#FFFFFF", "#B0B0B0"])
axes[2].set_title('CPU Usage Distribution', fontweight='bold', fontsize=22)
axes[2].set_xticklabels(['TS', 'FC'], fontsize=18)
axes[2].set_ylabel('CPU Usage (%)', fontsize=20)
axes[2].grid(True, alpha=0.3, linestyle=':')
axes[2].tick_params(axis='both', which='major', labelsize=18)

cpu_means = plot_df.groupby('Group')['CPU (%)'].mean()
for i, short in enumerate(['TS', 'FC']):
    axes[2].axhline(y=cpu_means[short], color='black', linestyle='--', 
                    alpha=0.7, linewidth=1.5)
    axes[2].text(i+0.4, cpu_means[short]+0.001, f'Î¼={cpu_means[short]:.4f}', 
                 fontsize=15, fontweight='bold', color='black',
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='#E0E0E0', edgecolor='black', alpha=0.7))

# åœ¨å›¾ä¸‹æ–¹ç»Ÿä¸€åŠ æ³¨è¯´æ˜
fig.text(0.5, -0.08, 'Abbreviations: TS=ToggleSwitch, FC=FyneCheckbox', ha='center', fontsize=16)

plt.tight_layout()

output_image = csv_path.stem + '_violin_plots.png'
plt.savefig(output_image, dpi=300, bbox_inches='tight')
print(f"âœ… å°æç´å›¾å·²ä¿å­˜: {output_image}")

plt.show()

# ==================== ç¬¬4æ­¥ï¼šç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ ====================
print(f"\nğŸ“Š è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ...")

def check_normality(data, name="data"):
    """æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£æ€åˆ†å¸ƒ"""
    if len(data) < 3:
        return True  # å°æ ·æœ¬å‡è®¾æ­£æ€
    
    try:
        stat, p = stats.shapiro(data)
        is_normal = p > 0.05
        print(f"   {name}: Shapiro-Wilk p={p:.4f}, {'normal' if is_normal else 'non-normal'}")
        return is_normal
    except Exception as e:
        print(f"   {name}: æ­£æ€æ€§æ£€éªŒå¤±è´¥ - {e}")
        return True  # é»˜è®¤å‡è®¾æ­£æ€

def perform_statistical_test(data1, data2, metric_name, data1_name="ToggleSwitch", data2_name="FyneCheckbox"):
    """æ‰§è¡Œå®Œæ•´çš„ç»Ÿè®¡æ£€éªŒ"""
    
    # ç§»é™¤NaNå€¼
    data1_clean = np.array(data1)[~np.isnan(data1)]
    data2_clean = np.array(data2)[~np.isnan(data2)]
    
    print(f"\n   [{metric_name}]")
    print(f"   {data1_name}: n={len(data1_clean)}, mean={np.mean(data1_clean):.4f}, std={np.std(data1_clean):.4f}")
    print(f"   {data2_name}: n={len(data2_clean)}, mean={np.mean(data2_clean):.4f}, std={np.std(data2_clean):.4f}")
    
    # 1. æ­£æ€æ€§æ£€éªŒ
    data1_normal = check_normality(data1_clean, f"{data1_name}_{metric_name}")
    data2_normal = check_normality(data2_clean, f"{data2_name}_{metric_name}")
    
    # 2. é€‰æ‹©æ£€éªŒæ–¹æ³•
    if data1_normal and data2_normal:
        # å‚æ•°æ£€éªŒï¼šç‹¬ç«‹æ ·æœ¬tæ£€éªŒ
        test_type = "Independent t-test"
        t_stat, p_value = stats.ttest_ind(data1_clean, data2_clean)
        
        # æ•ˆåº”é‡ï¼šCohen's d
        n1, n2 = len(data1_clean), len(data2_clean)
        pooled_std = np.sqrt(((n1-1)*np.var(data1_clean) + (n2-1)*np.var(data2_clean)) / (n1+n2-2))
        mean_diff = np.mean(data1_clean) - np.mean(data2_clean)
        cohens_d = mean_diff / pooled_std if pooled_std != 0 else 0
        
        effect_size = cohens_d
        effect_size_label = "Cohen's d"
        
    else:
        # éå‚æ•°æ£€éªŒï¼šMann-Whitney Uæ£€éªŒ
        test_type = "Mann-Whitney U test"
        u_stat, p_value = stats.mannwhitneyu(data1_clean, data2_clean)
        
        # æ•ˆåº”é‡ï¼šCliff's delta (è¿‘ä¼¼)
        mean_diff = np.mean(data1_clean) - np.mean(data2_clean)
        pooled_std = np.std(np.concatenate([data1_clean, data2_clean]))
        effect_size = mean_diff / pooled_std if pooled_std != 0 else 0
        effect_size_label = "Standardized Mean Difference"
    
    # 3. è®¡ç®—ç½®ä¿¡åŒºé—´
    mean_diff = np.mean(data1_clean) - np.mean(data2_clean)
    se_diff = np.sqrt(np.var(data1_clean)/len(data1_clean) + np.var(data2_clean)/len(data2_clean))
    ci_lower = mean_diff - 1.96 * se_diff
    ci_upper = mean_diff + 1.96 * se_diff
    
    # 4. æ•ˆåº”é‡è§£é‡Š
    if abs(effect_size) < 0.2:
        size_desc = "negligible"
    elif abs(effect_size) < 0.5:
        size_desc = "small"
    elif abs(effect_size) < 0.8:
        size_desc = "medium"
    else:
        size_desc = "large"
    
    # 5. æ˜¾è‘—æ€§åˆ¤æ–­
    significant = p_value < 0.05
    significance_desc = "SIGNIFICANT" if significant else "NOT SIGNIFICANT"
    
    return {
        'metric': metric_name,
        'test_type': test_type,
        'p_value': p_value,
        'mean_diff': mean_diff,
        'ci_95': (ci_lower, ci_upper),
        'effect_size': effect_size,
        'effect_size_label': effect_size_label,
        'effect_size_interpretation': size_desc,
        'significant': significant,
        'significance_desc': significance_desc,
        'data1_mean': np.mean(data1_clean),
        'data2_mean': np.mean(data2_clean),
        'data1_std': np.std(data1_clean),
        'data2_std': np.std(data2_clean),
        'n1': len(data1_clean),
        'n2': len(data2_clean)
    }

# æ‰§è¡Œä¸‰ä¸ªæŒ‡æ ‡çš„æ£€éªŒ
metrics_to_test = [
    ('fps', 'FPS'),
    ('memory_usage_mb', 'Memory Usage (MB)'),
    ('cpu_percent', 'CPU Usage (%)')
]

results = []
for col_name, display_name in metrics_to_test:
    if col_name in df.columns:
        result = perform_statistical_test(
            toggle_data[col_name].values,
            native_data[col_name].values,
            display_name
        )
        results.append(result)
    else:
        print(f"âš ï¸  è­¦å‘Š: åˆ— '{col_name}' ä¸å­˜åœ¨ï¼Œè·³è¿‡")

# ==================== ç¬¬5æ­¥ï¼šç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š ====================
print(f"\n" + "="*80)
print(" " * 20 + "STATISTICAL ANALYSIS REPORT")
print("="*80)

report_lines = []
for result in results:
    report_lines.append(f"\n{'='*60}")
    report_lines.append(f"METRIC: {result['metric']}")
    report_lines.append(f"{'='*60}")
    report_lines.append(f"Test Method: {result['test_type']}")
    report_lines.append(f"TS: mean={result['data1_mean']:.4f}, std={result['data1_std']:.4f}, n={result['n1']}")
    report_lines.append(f"FC: mean={result['data2_mean']:.4f}, std={result['data2_std']:.4f}, n={result['n2']}")
    report_lines.append(f"Mean Difference: {result['mean_diff']:.6f}")
    report_lines.append(f"95% Confidence Interval: [{result['ci_95'][0]:.6f}, {result['ci_95'][1]:.6f}]")
    report_lines.append(f"p-value: {result['p_value']:.10f} ({result['significance_desc']})")
    report_lines.append(f"Effect Size ({result['effect_size_label']}): {result['effect_size']:.4f} ({result['effect_size_interpretation']})")
    
    # è§£é‡Šç»“æœ
    if result['significant']:
        direction = "lower" if result['mean_diff'] < 0 else "higher"
        percent_diff = abs(result['mean_diff'] / result['data2_mean'] * 100)
        report_lines.append(f"CONCLUSION: Statistically significant difference ({direction} by {percent_diff:.2f}%)")
    else:
        report_lines.append(f"CONCLUSION: No statistically significant difference")

# æ‰“å°æŠ¥å‘Š
report_text = "\n".join(report_lines)
print(report_text)

# ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
report_filename = csv_path.stem + '_statistical_report.txt'
with open(report_filename, 'w', encoding='utf-8') as f:
    f.write(report_text)
print(f"\nâœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")

# ==================== ç¬¬6æ­¥ï¼šç”Ÿæˆæ±‡æ€»è¡¨æ ¼ ====================
print(f"\nğŸ“‹ æ€§èƒ½æŒ‡æ ‡æ±‡æ€»:")
print("-" * 90)
print(f"{'Metric':<20} {'TS':<15} {'FC':<15} {'Difference':<15} {'p-value':<12} {'Significant':<10}")
print("-" * 90)

for result in results:
    toggle_val = f"{result['data1_mean']:.4f}"
    native_val = f"{result['data2_mean']:.4f}"
    diff_val = f"{result['mean_diff']:.4f}"
    p_val = f"{result['p_value']:.6f}"
    sig = "âœ“" if result['significant'] else "âœ—"
    
    print(f"{result['metric']:<20} {toggle_val:<15} {native_val:<15} {diff_val:<15} {p_val:<12} {sig:<10}")

print("-" * 90)

# ==================== å®Œæˆ ====================
print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
print(f"   1. å°æç´å›¾: {output_image}")
print(f"   2. ç»Ÿè®¡æŠ¥å‘Š: {report_filename}")
print(f"   3. åŸå§‹æ•°æ®: {csv_filename}")